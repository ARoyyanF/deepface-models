{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13416fc",
   "metadata": {},
   "source": [
    "# Facial Recognition Model Performance Comparison\n",
    "\n",
    "This notebook provides a comprehensive framework for comparing facial recognition models using the DeepFace library. It's designed to be extensible for multiple datasets and includes future fine-tuning capabilities.\n",
    "\n",
    "## Features:\n",
    "- **Multi-Model Comparison**: Compare various DeepFace models (VGG-Face, Facenet, OpenFace, etc.)\n",
    "- **Extensible Dataset Support**: Currently supports LFW dataset with framework for adding others\n",
    "- **Performance Metrics**: Accuracy, Precision, Recall, F1-Score evaluation\n",
    "- **Visualization**: Comprehensive charts and performance comparisons\n",
    "- **Fine-tuning Ready**: Framework setup for future model customization\n",
    "\n",
    "## Dataset Structure:\n",
    "- LFW (Labeled Faces in the Wild) dataset in `dataset/lfw/` folder\n",
    "- Extensible structure for additional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ae732",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18502849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ARF\\my-repos\\magang\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "‚úÖ DeepFace imported successfully\n",
      "üìö All libraries imported successfully!\n",
      "üìÖ Notebook initialized on: 2025-07-23 19:25:59\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep learning and computer vision\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# DeepFace library for facial recognition\n",
    "try:\n",
    "    from deepface import DeepFace\n",
    "    print(\"‚úÖ DeepFace imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå DeepFace not found. Install using: pip install deepface\")\n",
    "\n",
    "# Additional utilities\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")\n",
    "print(f\"üìÖ Notebook initialized on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f10f18",
   "metadata": {},
   "source": [
    "## 2. Setup Configuration and Parameters\n",
    "\n",
    "Define configuration parameters for datasets, models, evaluation metrics, and file paths. This section creates an extensible structure for adding new datasets and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45eaf509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration setup complete!\n",
      "üìÅ Available datasets: ['lfw', 'asian_celeb']\n",
      "ü§ñ Available models: ['VGG-Face', 'Facenet', 'Facenet512', 'OpenFace', 'DeepFace', 'DeepID', 'ArcFace', 'Dlib', 'SFace', 'GhostFaceNet', 'Buffalo_L']\n",
      "üìä Evaluation metrics: ['accuracy', 'precision', 'recall', 'f1_score']\n",
      "üìè Distance metrics: ['cosine', 'euclidean', 'euclidean_l2']\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration class for dataset parameters\"\"\"\n",
    "    name: str\n",
    "    path: str\n",
    "    image_folder: str\n",
    "    pairs_file: Optional[str] = None\n",
    "    people_file: Optional[str] = None\n",
    "    train_pairs: Optional[str] = None\n",
    "    test_pairs: Optional[str] = None\n",
    "    dataset_type: str = 'structured'  # 'structured' (with CSV files) or 'folder_based' (folders only)\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration class for model parameters\"\"\"\n",
    "    name: str\n",
    "    backend: str\n",
    "    distance_metric: str = 'cosine'\n",
    "    enforce_detection: bool = True\n",
    "\n",
    "# Dataset configurations (extensible structure)\n",
    "DATASET_CONFIGS = {\n",
    "    'lfw': DatasetConfig(\n",
    "        name='LFW',\n",
    "        path='dataset/lfw',\n",
    "        image_folder='lfw-deepfunneled/lfw-deepfunneled',\n",
    "        pairs_file='pairs.csv',\n",
    "        people_file='people.csv',\n",
    "        train_pairs='matchpairsDevTrain.csv',\n",
    "        test_pairs='matchpairsDevTest.csv',\n",
    "        dataset_type='structured'\n",
    "    ),\n",
    "    'asian_celeb': DatasetConfig(\n",
    "        name='Asian Celebrity',\n",
    "        path='dataset/asian_celeb_112x112_folders',\n",
    "        image_folder='.',  # Images are directly in the dataset folder\n",
    "        dataset_type='folder_based'\n",
    "    ),\n",
    "    # Future datasets can be added here\n",
    "    # 'celeba': DatasetConfig(...),\n",
    "    # 'vggface2': DatasetConfig(...),\n",
    "}\n",
    "\n",
    "# Available DeepFace models for comparison\n",
    "DEEPFACE_MODELS = [\n",
    "    ModelConfig('VGG-Face', 'VGG-Face'),\n",
    "    ModelConfig('Facenet', 'Facenet'),\n",
    "    ModelConfig('Facenet512', 'Facenet512'),\n",
    "    ModelConfig('OpenFace', 'OpenFace'),\n",
    "    ModelConfig('DeepFace', 'DeepFace'),\n",
    "    ModelConfig('DeepID', 'DeepID'),\n",
    "    ModelConfig('ArcFace', 'ArcFace'),\n",
    "    ModelConfig('Dlib', 'Dlib'),\n",
    "    ModelConfig('SFace', 'SFace'),\n",
    "    ModelConfig('GhostFaceNet', 'GhostFaceNet'),\n",
    "    ModelConfig('Buffalo_L', 'Buffalo_L'),\n",
    "]\n",
    "\n",
    "# Evaluation metrics\n",
    "METRICS = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "# Distance metrics for face verification\n",
    "DISTANCE_METRICS = ['cosine', 'euclidean', 'euclidean_l2']\n",
    "\n",
    "# Results storage paths\n",
    "RESULTS_DIR = Path('results')\n",
    "MODELS_DIR = Path('models')\n",
    "PLOTS_DIR = Path('plots')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [RESULTS_DIR, MODELS_DIR, PLOTS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Global configuration\n",
    "CONFIG = {\n",
    "    'max_samples': 1,  # For testing; set to None for full dataset\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'image_size': (224, 224),\n",
    "    'batch_size': 32,\n",
    "    'verbose': True,\n",
    "    'max_verification_pair':1,\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration setup complete!\")\n",
    "print(f\"üìÅ Available datasets: {list(DATASET_CONFIGS.keys())}\")\n",
    "print(f\"ü§ñ Available models: {[model.name for model in DEEPFACE_MODELS]}\")\n",
    "print(f\"üìä Evaluation metrics: {METRICS}\")\n",
    "print(f\"üìè Distance metrics: {DISTANCE_METRICS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6c669",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing\n",
    "\n",
    "Load the LFW dataset and create extensible functions to load other facial recognition datasets. This section implements a preprocessing pipeline for image normalization and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89997c9b",
   "metadata": {},
   "source": [
    "### üìã Dataset Selection Instructions\n",
    "\n",
    "This notebook supports multiple dataset types:\n",
    "\n",
    "1. **LFW Dataset** (`'lfw'`): Structured dataset with predefined CSV files\n",
    "   - Path: `dataset/lfw/`\n",
    "   - Contains: `pairs.csv`, `people.csv`, training/test splits\n",
    "   - Image folder: `lfw-deepfunneled/lfw-deepfunneled/`\n",
    "\n",
    "2. **Asian Celebrity Dataset** (`'asian_celeb'`): Folder-based dataset\n",
    "   - Path: `dataset/asian_celeb_112x112_folders/`\n",
    "   - Contains: Only folders named by person identity\n",
    "   - Image folder: `.` (root of dataset)\n",
    "   - Pairs are generated dynamically from folder structure\n",
    "\n",
    "**To switch datasets**: Change the `SELECTED_DATASET` variable in the next cell and re-run from that cell onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab761610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing dataset loader...\n",
      "üîÑ Initializing LFW dataset loader...\n",
      "   Dataset type: structured\n",
      "   Path: dataset/lfw\n",
      "\n",
      "üìä Loading dataset information...\n",
      "‚úÖ Loaded 6000 pairs from LFW\n",
      "‚úÖ Loaded 5758 people from LFW\n",
      "\n",
      "üñºÔ∏è Loading image paths...\n",
      "‚úÖ Found 1 images in LFW\n",
      "\n",
      "üìà Dataset Summary:\n",
      "  - Dataset: LFW\n",
      "  - Type: structured\n",
      "  - Total pairs loaded: 6000\n",
      "  - Total people: 5758\n",
      "  - Sample images: 1\n",
      "\n",
      "üîÑ Creating verification pairs...\n",
      "‚úÖ Found 13233 images in LFW\n",
      "‚úÖ Created 0 verification pairs (0 positive, 0 negative)\n"
     ]
    }
   ],
   "source": [
    "class DatasetLoader:\n",
    "    \"\"\"Extensible dataset loader for facial recognition datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_config: DatasetConfig):\n",
    "        self.config = dataset_config\n",
    "        self.base_path = Path(dataset_config.path)\n",
    "        self.image_path = self.base_path / dataset_config.image_folder\n",
    "        \n",
    "    def load_pairs_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load pairs data for face verification\"\"\"\n",
    "        if self.config.dataset_type == 'folder_based':\n",
    "            print(f\"üìÇ {self.config.name} is folder-based. Pairs will be generated dynamically.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            pairs_file = self.base_path / self.config.pairs_file\n",
    "            if pairs_file.exists():\n",
    "                pairs_df = pd.read_csv(pairs_file)\n",
    "                print(f\"‚úÖ Loaded {len(pairs_df)} pairs from {self.config.name}\")\n",
    "                return pairs_df\n",
    "            else:\n",
    "                print(f\"‚ùå Pairs file not found: {pairs_file}\")\n",
    "                return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading pairs data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_people_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load people/identity data\"\"\"\n",
    "        if self.config.dataset_type == 'folder_based':\n",
    "            print(f\"üìÇ {self.config.name} is folder-based. People data will be generated from folders.\")\n",
    "            return self._generate_people_data_from_folders()\n",
    "        \n",
    "        try:\n",
    "            people_file = self.base_path / self.config.people_file\n",
    "            if people_file.exists():\n",
    "                people_df = pd.read_csv(people_file)\n",
    "                print(f\"‚úÖ Loaded {len(people_df)} people from {self.config.name}\")\n",
    "                return people_df\n",
    "            else:\n",
    "                print(f\"‚ùå People file not found: {people_file}\")\n",
    "                return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading people data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _generate_people_data_from_folders(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate people data from folder structure\"\"\"\n",
    "        try:\n",
    "            if not self.image_path.exists():\n",
    "                print(f\"‚ùå Image folder not found: {self.image_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            people_data = []\n",
    "            person_folders = [f for f in self.image_path.iterdir() if f.is_dir()]\n",
    "            \n",
    "            for person_folder in person_folders:\n",
    "                # Count images in each person's folder\n",
    "                image_count = 0\n",
    "                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "                    image_count += len(list(person_folder.glob(ext)))\n",
    "                \n",
    "                if image_count > 0:\n",
    "                    people_data.append({\n",
    "                        'name': person_folder.name,\n",
    "                        'folder': person_folder.name,\n",
    "                        'image_count': image_count\n",
    "                    })\n",
    "            \n",
    "            people_df = pd.DataFrame(people_data)\n",
    "            print(f\"‚úÖ Generated people data from {len(people_df)} folders\")\n",
    "            return people_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating people data from folders: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_image_paths(self, max_samples: Optional[int] = None) -> List[str]:\n",
    "        \"\"\"Get all available image paths\"\"\"\n",
    "        image_paths = []\n",
    "        \n",
    "        if not self.image_path.exists():\n",
    "            print(f\"‚ùå Image folder not found: {self.image_path}\")\n",
    "            return image_paths\n",
    "        \n",
    "        # Recursively find all image files\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "            image_paths.extend(list(self.image_path.rglob(ext)))\n",
    "        \n",
    "        # Convert to strings and limit samples if specified\n",
    "        image_paths = [str(path) for path in image_paths]\n",
    "        \n",
    "        if max_samples and len(image_paths) > max_samples:\n",
    "            # For folder-based datasets, try to maintain diversity by sampling from different people\n",
    "            if self.config.dataset_type == 'folder_based':\n",
    "                image_paths = self._sample_diverse_images(image_paths, max_samples)\n",
    "            else:\n",
    "                image_paths = image_paths[:max_samples]\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(image_paths)} images in {self.config.name}\")\n",
    "        return image_paths\n",
    "    \n",
    "    def _sample_diverse_images(self, image_paths: List[str], max_samples: int) -> List[str]:\n",
    "        \"\"\"Sample images to maintain diversity across different people\"\"\"\n",
    "        # Group images by person (folder name)\n",
    "        person_images = {}\n",
    "        for path in image_paths:\n",
    "            person_name = Path(path).parent.name\n",
    "            if person_name not in person_images:\n",
    "                person_images[person_name] = []\n",
    "            person_images[person_name].append(path)\n",
    "        \n",
    "        # Calculate how many images per person to maintain diversity\n",
    "        num_people = len(person_images)\n",
    "        images_per_person = max(1, max_samples // num_people)\n",
    "        \n",
    "        sampled_images = []\n",
    "        for person_name, person_paths in person_images.items():\n",
    "            # Sample up to images_per_person from each person\n",
    "            sample_count = min(images_per_person, len(person_paths))\n",
    "            sampled_images.extend(np.random.choice(person_paths, sample_count, replace=False))\n",
    "            \n",
    "            if len(sampled_images) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        return sampled_images[:max_samples]\n",
    "    \n",
    "    def get_people_folders(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Get mapping of person names to their image paths\"\"\"\n",
    "        people_folders = {}\n",
    "        \n",
    "        if not self.image_path.exists():\n",
    "            print(f\"‚ùå Image folder not found: {self.image_path}\")\n",
    "            return people_folders\n",
    "        \n",
    "        # Get all person folders\n",
    "        person_folders = [f for f in self.image_path.iterdir() if f.is_dir()]\n",
    "        \n",
    "        for person_folder in person_folders:\n",
    "            person_name = person_folder.name\n",
    "            image_paths = []\n",
    "            \n",
    "            # Find all images in this person's folder\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "                image_paths.extend([str(p) for p in person_folder.glob(ext)])\n",
    "            \n",
    "            if image_paths:\n",
    "                people_folders[person_name] = image_paths\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(people_folders)} people with images\")\n",
    "        return people_folders\n",
    "    \n",
    "    def validate_image(self, image_path: str) -> bool:\n",
    "        \"\"\"Validate if image can be loaded and processed\"\"\"\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            return img is not None and img.shape[0] > 0 and img.shape[1] > 0\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def preprocess_image(self, image_path: str, target_size: Tuple[int, int] = (224, 224)) -> Optional[np.ndarray]:\n",
    "        \"\"\"Preprocess image for model input\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                return None\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize image\n",
    "            img = cv2.resize(img, target_size)\n",
    "            \n",
    "            # Normalize pixel values\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            \n",
    "            return img\n",
    "        except Exception as e:\n",
    "            if CONFIG['verbose']:\n",
    "                print(f\"‚ùå Error preprocessing {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def create_verification_pairs(dataset_loader: DatasetLoader, num_pairs: int = 1000) -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"Create pairs for face verification (positive and negative pairs)\"\"\"\n",
    "    \n",
    "    if dataset_loader.config.dataset_type == 'folder_based':\n",
    "        return create_verification_pairs_from_folders(dataset_loader, num_pairs)\n",
    "    else:\n",
    "        return create_verification_pairs_from_images(dataset_loader, num_pairs)\n",
    "\n",
    "def create_verification_pairs_from_folders(dataset_loader: DatasetLoader, num_pairs: int = 1000) -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"Create verification pairs from folder-based dataset structure\"\"\"\n",
    "    people_folders = dataset_loader.get_people_folders()\n",
    "    \n",
    "    if len(people_folders) < 2:\n",
    "        print(\"‚ùå Not enough people to create pairs\")\n",
    "        return []\n",
    "    \n",
    "    pairs = []\n",
    "    people_names = list(people_folders.keys())\n",
    "    \n",
    "    # Create positive pairs (same person)\n",
    "    positive_pairs = 0\n",
    "    target_positive = num_pairs // 2\n",
    "    \n",
    "    for person_name in people_names:\n",
    "        person_images = people_folders[person_name]\n",
    "        \n",
    "        if len(person_images) < 2:\n",
    "            continue  # Need at least 2 images to create a positive pair\n",
    "        \n",
    "        # Create multiple positive pairs for this person\n",
    "        max_pairs_per_person = min(5, target_positive - positive_pairs)\n",
    "        \n",
    "        for _ in range(max_pairs_per_person):\n",
    "            if positive_pairs >= target_positive:\n",
    "                break\n",
    "            \n",
    "            # Randomly select 2 different images of the same person\n",
    "            img1, img2 = np.random.choice(person_images, 2, replace=False)\n",
    "            pairs.append((img1, img2, 1))  # 1 = positive pair\n",
    "            positive_pairs += 1\n",
    "    \n",
    "    # Create negative pairs (different persons)\n",
    "    negative_pairs = 0\n",
    "    target_negative = num_pairs // 2\n",
    "    \n",
    "    while negative_pairs < target_negative:\n",
    "        # Randomly select two different people\n",
    "        person1, person2 = np.random.choice(people_names, 2, replace=False)\n",
    "        \n",
    "        # Select one random image from each person\n",
    "        img1 = np.random.choice(people_folders[person1])\n",
    "        img2 = np.random.choice(people_folders[person2])\n",
    "        \n",
    "        pairs.append((img1, img2, 0))  # 0 = negative pair\n",
    "        negative_pairs += 1\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(pairs)} verification pairs ({positive_pairs} positive, {negative_pairs} negative)\")\n",
    "    return pairs\n",
    "\n",
    "def create_verification_pairs_from_images(dataset_loader: DatasetLoader, num_pairs: int = 1000) -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"Create verification pairs from image list (original method for structured datasets)\"\"\"\n",
    "    image_paths = dataset_loader.get_image_paths()\n",
    "    \n",
    "    if len(image_paths) < 2:\n",
    "        print(\"‚ùå Not enough images to create pairs\")\n",
    "        return []\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    # Create positive pairs (same person)\n",
    "    positive_pairs = 0\n",
    "    for i, path1 in enumerate(image_paths):\n",
    "        if positive_pairs >= num_pairs // 2:\n",
    "            break\n",
    "        \n",
    "        # Extract person name from path (assuming folder structure: person_name/image.jpg)\n",
    "        person1 = Path(path1).parent.name\n",
    "        \n",
    "        for j, path2 in enumerate(image_paths[i+1:], i+1):\n",
    "            person2 = Path(path2).parent.name\n",
    "            \n",
    "            if person1 == person2:  # Same person\n",
    "                pairs.append((path1, path2, 1))  # 1 = positive pair\n",
    "                positive_pairs += 1\n",
    "                break\n",
    "    \n",
    "    # Create negative pairs (different persons)\n",
    "    negative_pairs = 0\n",
    "    while negative_pairs < num_pairs // 2 and len(image_paths) >= 2:\n",
    "        path1 = np.random.choice(image_paths)\n",
    "        path2 = np.random.choice(image_paths)\n",
    "        \n",
    "        person1 = Path(path1).parent.name\n",
    "        person2 = Path(path2).parent.name\n",
    "        \n",
    "        if person1 != person2:  # Different persons\n",
    "            pairs.append((path1, path2, 0))  # 0 = negative pair\n",
    "            negative_pairs += 1\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(pairs)} verification pairs ({positive_pairs} positive, {negative_pairs} negative)\")\n",
    "    return pairs\n",
    "\n",
    "# Function to select and initialize dataset\n",
    "def select_dataset(dataset_name: str = 'lfw') -> DatasetLoader:\n",
    "    \"\"\"Select and initialize a dataset loader\"\"\"\n",
    "    if dataset_name not in DATASET_CONFIGS:\n",
    "        print(f\"‚ùå Dataset '{dataset_name}' not found. Available datasets: {list(DATASET_CONFIGS.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    config = DATASET_CONFIGS[dataset_name]\n",
    "    print(f\"üîÑ Initializing {config.name} dataset loader...\")\n",
    "    print(f\"   Dataset type: {config.dataset_type}\")\n",
    "    print(f\"   Path: {config.path}\")\n",
    "    \n",
    "    return DatasetLoader(config)\n",
    "\n",
    "# Initialize dataset loader - you can change this to 'asian_celeb' for the Asian Celebrity dataset\n",
    "SELECTED_DATASET = 'lfw'  # Change to 'asian_celeb' to use Asian Celebrity dataset\n",
    "\n",
    "print(\"üîÑ Initializing dataset loader...\")\n",
    "dataset_loader = select_dataset(SELECTED_DATASET)\n",
    "\n",
    "if dataset_loader:\n",
    "    # Load dataset information\n",
    "    print(\"\\nüìä Loading dataset information...\")\n",
    "    dataset_pairs = dataset_loader.load_pairs_data()\n",
    "    dataset_people = dataset_loader.load_people_data()\n",
    "    \n",
    "    # Get sample of image paths\n",
    "    print(\"\\nüñºÔ∏è Loading image paths...\")\n",
    "    sample_images = dataset_loader.get_image_paths(max_samples=CONFIG['max_samples'])\n",
    "    \n",
    "    print(f\"\\nüìà Dataset Summary:\")\n",
    "    print(f\"  - Dataset: {dataset_loader.config.name}\")\n",
    "    print(f\"  - Type: {dataset_loader.config.dataset_type}\")\n",
    "    print(f\"  - Total pairs loaded: {len(dataset_pairs) if not dataset_pairs.empty else 'Generated dynamically'}\")\n",
    "    print(f\"  - Total people: {len(dataset_people) if not dataset_people.empty else 0}\")\n",
    "    print(f\"  - Sample images: {len(sample_images)}\")\n",
    "    \n",
    "    # Create verification pairs for testing\n",
    "    if sample_images:\n",
    "        print(\"\\nüîÑ Creating verification pairs...\")\n",
    "        verification_pairs = create_verification_pairs(dataset_loader, num_pairs=min(CONFIG['max_verification_pair'], len(sample_images)))\n",
    "    else:\n",
    "        verification_pairs = []\n",
    "        print(\"‚ùå No images found for creating verification pairs\")\n",
    "else:\n",
    "    verification_pairs = []\n",
    "    dataset_pairs = pd.DataFrame()\n",
    "    dataset_people = pd.DataFrame()\n",
    "    sample_images = []\n",
    "    print(\"‚ùå Failed to initialize dataset loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888968c2",
   "metadata": {},
   "source": [
    "## 4. Load Available DeepFace Models\n",
    "\n",
    "Initialize and load different DeepFace models for comparison. This section handles model loading and provides a unified interface for different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee7fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Initializing model manager...\n",
      "üîç Checking model availability...\n",
      "‚úÖ VGG-Face: Added to available models\n",
      "‚úÖ Facenet: Added to available models\n",
      "‚úÖ Facenet512: Added to available models\n",
      "‚ö†Ô∏è OpenFace: Skipped (may require additional setup)\n",
      "‚ö†Ô∏è DeepFace: Skipped (may require additional setup)\n",
      "‚úÖ DeepID: Added to available models\n",
      "‚úÖ ArcFace: Added to available models\n",
      "‚úÖ Dlib: Added to available models\n",
      "‚úÖ SFace: Added to available models\n",
      "‚úÖ GhostFaceNet: Added to available models\n",
      "‚ö†Ô∏è Buffalo_L: Skipped (may require additional setup)\n",
      "\n",
      "üìã Total available models: 8\n",
      "\n",
      "üìä Model Information:\n",
      "--------------------------------------------------------------------------------\n",
      "Model           Backend         Input Shape  Description                   \n",
      "--------------------------------------------------------------------------------\n",
      "VGG-Face        VGG-Face        224x224x3    CNN architecture trained on VGGFace dataset\n",
      "Facenet         Facenet         224x224x3    Google's FaceNet with triplet loss training\n",
      "Facenet512      Facenet512      224x224x3    FaceNet variant with 512-dimensional embeddings\n",
      "DeepID          DeepID          224x224x3    Deep Learning Face Representation\n",
      "ArcFace         ArcFace         224x224x3    Additive Angular Margin Loss for face recognition\n",
      "Dlib            Dlib            224x224x3    Dlib's ResNet-based face recognition\n",
      "SFace           SFace           224x224x3    Sigmoid-constrained Face Recognition\n",
      "GhostFaceNet    GhostFaceNet    224x224x3    GhostFaceNet for efficient face recognition\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Manager class for DeepFace models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available_models = {}\n",
    "        self.model_info = {}\n",
    "        \n",
    "    def test_model_availability(self, model_config: ModelConfig) -> bool:\n",
    "        \"\"\"Test if a model can be loaded successfully\"\"\"\n",
    "        try:\n",
    "            print(f\"üîÑ Testing {model_config.name}...\")\n",
    "            \n",
    "            # Try to load the model by running a simple verification\n",
    "            # We'll use a dummy verification to test model loading\n",
    "            result = DeepFace.verify(\n",
    "                img1_path=\"dummy\",  # This will fail but help us check if model exists\n",
    "                img2_path=\"dummy\",\n",
    "                model_name=model_config.backend,\n",
    "                enforce_detection=False\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"no such file\" in error_msg or \"not found\" in error_msg:\n",
    "                print(f\"‚ùå {model_config.name}: Model files not available\")\n",
    "                return False\n",
    "            elif \"dummy\" in error_msg:\n",
    "                # Expected error due to dummy paths - model is available\n",
    "                print(f\"‚úÖ {model_config.name}: Available\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå {model_config.name}: Error - {e}\")\n",
    "                return False\n",
    "    \n",
    "    def load_available_models(self) -> Dict[str, ModelConfig]:\n",
    "        \"\"\"Load all available models\"\"\"\n",
    "        print(\"üîç Checking model availability...\")\n",
    "        \n",
    "        # For demonstration, we'll assume these models are commonly available\n",
    "        # In practice, you might want to test each one\n",
    "        commonly_available = ['VGG-Face', 'Facenet', 'Facenet512', 'DeepID', 'ArcFace', 'Dlib', 'SFace', 'GhostFaceNet']\n",
    "        \n",
    "        for model_config in DEEPFACE_MODELS:\n",
    "            if model_config.backend in commonly_available:\n",
    "                self.available_models[model_config.name] = model_config\n",
    "                print(f\"‚úÖ {model_config.name}: Added to available models\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è {model_config.name}: Skipped (may require additional setup)\")\n",
    "        \n",
    "        print(f\"\\nüìã Total available models: {len(self.available_models)}\")\n",
    "        return self.available_models\n",
    "    \n",
    "    def get_model_info(self, model_name: str) -> Dict:\n",
    "        \"\"\"Get information about a specific model\"\"\"\n",
    "        if model_name not in self.available_models:\n",
    "            return {}\n",
    "        \n",
    "        model_config = self.available_models[model_name]\n",
    "        \n",
    "        return {\n",
    "            'name': model_config.name,\n",
    "            'backend': model_config.backend,\n",
    "            'distance_metric': model_config.distance_metric,\n",
    "            'input_shape': self._get_input_shape(model_config.backend),\n",
    "            'description': self._get_model_description(model_config.backend)\n",
    "        }\n",
    "    \n",
    "    def _get_input_shape(self, backend: str) -> str:\n",
    "        \"\"\"Get typical input shape for the model\"\"\"\n",
    "        # Updated to reflect standard 224x224 input for most models\n",
    "        input_shapes = {\n",
    "            'VGG-Face': '224x224x3',\n",
    "            'Facenet': '224x224x3',\n",
    "            'Facenet512': '224x224x3',\n",
    "            'OpenFace': '224x224x3',\n",
    "            'DeepFace': '224x224x3',\n",
    "            'DeepID': '224x224x3',\n",
    "            'ArcFace': '224x224x3',\n",
    "            'Dlib': '224x224x3',\n",
    "            'SFace': '224x224x3',\n",
    "            'GhostFaceNet': '224x224x3',\n",
    "            'Buffalo_L': '224x224x3'\n",
    "        }\n",
    "        return input_shapes.get(backend, 'Unknown')\n",
    "    \n",
    "    def _get_model_description(self, backend: str) -> str:\n",
    "        \"\"\"Get description for the model\"\"\"\n",
    "        descriptions = {\n",
    "            'VGG-Face': 'CNN architecture trained on VGGFace dataset',\n",
    "            'Facenet': 'Google\\'s FaceNet with triplet loss training',\n",
    "            'Facenet512': 'FaceNet variant with 512-dimensional embeddings',\n",
    "            'OpenFace': 'Carnegie Mellon\\'s OpenFace implementation',\n",
    "            'DeepFace': 'Facebook\\'s DeepFace architecture',\n",
    "            'DeepID': 'Deep Learning Face Representation',\n",
    "            'ArcFace': 'Additive Angular Margin Loss for face recognition',\n",
    "            'Dlib': 'Dlib\\'s ResNet-based face recognition',\n",
    "            'SFace': 'Sigmoid-constrained Face Recognition',\n",
    "            'GhostFaceNet': 'GhostFaceNet for efficient face recognition',\n",
    "            'Buffalo_L': 'Buffalo_L for large-scale face recognition'\n",
    "        }\n",
    "        return descriptions.get(backend, 'Face recognition model')\n",
    "\n",
    "# Initialize model manager\n",
    "print(\"ü§ñ Initializing model manager...\")\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# Load available models\n",
    "available_models = model_manager.load_available_models()\n",
    "\n",
    "# Display model information\n",
    "print(\"\\nüìä Model Information:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<15} {'Backend':<15} {'Input Shape':<12} {'Description':<30}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name in available_models:\n",
    "    info = model_manager.get_model_info(model_name)\n",
    "    print(f\"{info['name']:<15} {info['backend']:<15} {info['input_shape']:<12} {info['description']:<30}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66050c",
   "metadata": {},
   "source": [
    "## 5. Model Performance Evaluation\n",
    "\n",
    "Implement evaluation functions to test model accuracy, precision, recall, and F1-score. This section runs performance tests across all loaded models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d23546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Initializing model evaluator...\n",
      "‚ö†Ô∏è No verification pairs available for evaluation\n",
      "   Make sure the dataset is properly loaded and images are accessible\n"
     ]
    }
   ],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation class\"\"\"\n",
    "    \n",
    "    def __init__(self, model_manager: ModelManager):\n",
    "        self.model_manager = model_manager\n",
    "        self.results = {}\n",
    "        \n",
    "    def verify_face_pair(self, img1_path: str, img2_path: str, model_name: str, \n",
    "                        distance_metric: str = 'cosine') -> Tuple[bool, float, Dict]:\n",
    "        \"\"\"Verify if two faces belong to the same person\"\"\"\n",
    "        try:\n",
    "            model_config = self.model_manager.available_models[model_name]\n",
    "            \n",
    "            result = DeepFace.verify(\n",
    "                img1_path=img1_path,\n",
    "                img2_path=img2_path,\n",
    "                model_name=model_config.backend,\n",
    "                distance_metric=distance_metric,\n",
    "                enforce_detection=False  # Set to False to handle detection failures gracefully\n",
    "            )\n",
    "            \n",
    "            return result['verified'], result['distance'], result\n",
    "        except Exception as e:\n",
    "            if CONFIG['verbose']:\n",
    "                print(f\"‚ùå Verification failed for {model_name}: {e}\")\n",
    "            return False, float('inf'), {'error': str(e)}\n",
    "    \n",
    "    def evaluate_model_on_pairs(self, model_name: str, pairs: List[Tuple[str, str, int]], \n",
    "                               distance_metric: str = 'cosine') -> Dict:\n",
    "        \"\"\"Evaluate a model on a set of verification pairs\"\"\"\n",
    "        print(f\"üîÑ Evaluating {model_name} on {len(pairs)} pairs...\")\n",
    "        \n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "        distances = []\n",
    "        processing_times = []\n",
    "        errors = 0\n",
    "        \n",
    "        for i, (img1_path, img2_path, label) in enumerate(pairs):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"  Progress: {i}/{len(pairs)} pairs processed\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            verified, distance, result = self.verify_face_pair(\n",
    "                img1_path, img2_path, model_name, distance_metric\n",
    "            )\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            if 'error' not in result:\n",
    "                predictions.append(1 if verified else 0)\n",
    "                ground_truth.append(label)\n",
    "                distances.append(distance)\n",
    "                processing_times.append(processing_time)\n",
    "            else:\n",
    "                errors += 1\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            print(f\"‚ùå No successful predictions for {model_name}\")\n",
    "            return {\n",
    "                'model': model_name,\n",
    "                'accuracy': 0.0,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'avg_processing_time': 0.0,\n",
    "                'error_rate': 1.0,\n",
    "                'total_pairs': len(pairs),\n",
    "                'successful_pairs': 0\n",
    "            }\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(ground_truth, predictions)\n",
    "        precision = precision_score(ground_truth, predictions, zero_division=0)\n",
    "        recall = recall_score(ground_truth, predictions, zero_division=0)\n",
    "        f1 = f1_score(ground_truth, predictions, zero_division=0)\n",
    "        \n",
    "        avg_processing_time = np.mean(processing_times)\n",
    "        error_rate = errors / len(pairs)\n",
    "        \n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'avg_processing_time': avg_processing_time,\n",
    "            'error_rate': error_rate,\n",
    "            'total_pairs': len(pairs),\n",
    "            'successful_pairs': len(predictions),\n",
    "            'distances': distances,\n",
    "            'predictions': predictions,\n",
    "            'ground_truth': ground_truth\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {model_name} evaluation complete:\")\n",
    "        print(f\"  - Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"  - Precision: {precision:.3f}\")\n",
    "        print(f\"  - Recall: {recall:.3f}\")\n",
    "        print(f\"  - F1-Score: {f1:.3f}\")\n",
    "        print(f\"  - Avg Processing Time: {avg_processing_time:.3f}s\")\n",
    "        print(f\"  - Error Rate: {error_rate:.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_all_models(self, pairs: List[Tuple[str, str, int]], \n",
    "                           distance_metric: str = 'cosine') -> Dict:\n",
    "        \"\"\"Evaluate all available models\"\"\"\n",
    "        print(f\"üöÄ Starting evaluation of {len(self.model_manager.available_models)} models...\")\n",
    "        print(f\"üìä Total pairs to process: {len(pairs)}\")\n",
    "        print(f\"üìè Distance metric: {distance_metric}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name in self.model_manager.available_models:\n",
    "            print(f\"\\nüîÑ Evaluating {model_name}...\")\n",
    "            try:\n",
    "                results = self.evaluate_model_on_pairs(model_name, pairs, distance_metric)\n",
    "                all_results[model_name] = results\n",
    "                print(f\"‚úÖ {model_name} evaluation completed successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {model_name} evaluation failed: {e}\")\n",
    "                all_results[model_name] = {\n",
    "                    'model': model_name,\n",
    "                    'accuracy': 0.0,\n",
    "                    'precision': 0.0,\n",
    "                    'recall': 0.0,\n",
    "                    'f1_score': 0.0,\n",
    "                    'avg_processing_time': 0.0,\n",
    "                    'error_rate': 1.0,\n",
    "                    'total_pairs': len(pairs),\n",
    "                    'successful_pairs': 0,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        self.results = all_results\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üéâ All model evaluations completed!\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def get_summary_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Convert results to a pandas DataFrame for easy analysis\"\"\"\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        summary_data = []\n",
    "        for model_name, results in self.results.items():\n",
    "            summary_data.append({\n",
    "                'Model': model_name,\n",
    "                'Accuracy': results.get('accuracy', 0.0),\n",
    "                'Precision': results.get('precision', 0.0),\n",
    "                'Recall': results.get('recall', 0.0),\n",
    "                'F1-Score': results.get('f1_score', 0.0),\n",
    "                'Avg Processing Time (s)': results.get('avg_processing_time', 0.0),\n",
    "                'Error Rate': results.get('error_rate', 1.0),\n",
    "                'Successful Pairs': results.get('successful_pairs', 0),\n",
    "                'Total Pairs': results.get('total_pairs', 0)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "# Initialize evaluator\n",
    "print(\"üìä Initializing model evaluator...\")\n",
    "evaluator = ModelEvaluator(model_manager)\n",
    "\n",
    "# Run evaluation if we have verification pairs\n",
    "if verification_pairs and len(verification_pairs) > 0:\n",
    "    print(f\"\\nüöÄ Starting model evaluation on {len(verification_pairs)} pairs...\")\n",
    "    \n",
    "    # Run evaluation (this may take some time)\n",
    "    start_time = time.time()\n",
    "    evaluation_results = evaluator.evaluate_all_models(verification_pairs)\n",
    "    evaluation_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Evaluation completed in {evaluation_time/60.0:.2f} minutes\")\n",
    "    # Get summary DataFrame\n",
    "    results_df = evaluator.get_summary_dataframe()\n",
    "    \n",
    "    print(\"\\nüìã Evaluation Summary:\")\n",
    "    print(results_df.round(3))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No verification pairs available for evaluation\")\n",
    "    print(\"   Make sure the dataset is properly loaded and images are accessible\")\n",
    "    evaluation_results = {}\n",
    "    results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f83fe",
   "metadata": {},
   "source": [
    "## 6. Results Comparison and Visualization\n",
    "\n",
    "Create comprehensive visualizations comparing model performance metrics. This section generates charts, confusion matrices, and performance tables for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bfbfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No results available for visualization\n",
      "   Please run the model evaluation first\n"
     ]
    }
   ],
   "source": [
    "class ResultsVisualizer:\n",
    "    \"\"\"Comprehensive visualization class for model comparison results\"\"\"\n",
    "    \n",
    "    def __init__(self, results_df: pd.DataFrame, evaluation_results: Dict):\n",
    "        self.results_df = results_df\n",
    "        self.evaluation_results = evaluation_results\n",
    "        self.fig_size = (15, 10)\n",
    "        \n",
    "    def plot_performance_metrics(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot comparison of key performance metrics\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"‚ùå No results to visualize\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=self.fig_size)\n",
    "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "        colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']\n",
    "        \n",
    "        for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "            ax = axes[i//2, i%2]\n",
    "            \n",
    "            # Sort by metric value for better visualization\n",
    "            sorted_df = self.results_df.sort_values(metric, ascending=True)\n",
    "            \n",
    "            bars = ax.barh(sorted_df['Model'], sorted_df[metric], color=color, alpha=0.7)\n",
    "            ax.set_xlabel(metric)\n",
    "            ax.set_title(f'{metric} by Model')\n",
    "            ax.set_xlim(0, 1)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, sorted_df[metric]):\n",
    "                ax.text(value + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{value:.3f}', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"‚úÖ Performance metrics plot saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_processing_time_comparison(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot processing time comparison\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"‚ùå No results to visualize\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Sort by processing time\n",
    "        sorted_df = self.results_df.sort_values('Avg Processing Time (s)', ascending=True)\n",
    "        \n",
    "        bars = plt.bar(sorted_df['Model'], sorted_df['Avg Processing Time (s)'], \n",
    "                      color='lightblue', alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Average Processing Time (seconds)')\n",
    "        plt.title('Model Processing Time Comparison')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, sorted_df['Avg Processing Time (s)']):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{value:.3f}s', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"‚úÖ Processing time plot saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_accuracy_vs_speed(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot accuracy vs processing speed scatter plot\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"‚ùå No results to visualize\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create scatter plot\n",
    "        scatter = plt.scatter(self.results_df['Avg Processing Time (s)'], \n",
    "                            self.results_df['Accuracy'],\n",
    "                            s=100, alpha=0.7, c=range(len(self.results_df)), \n",
    "                            cmap='viridis')\n",
    "        \n",
    "        # Add model labels\n",
    "        for i, model in enumerate(self.results_df['Model']):\n",
    "            plt.annotate(model, \n",
    "                        (self.results_df.iloc[i]['Avg Processing Time (s)'], \n",
    "                         self.results_df.iloc[i]['Accuracy']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        plt.xlabel('Average Processing Time (seconds)')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy vs Processing Speed Trade-off')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add quadrant labels\n",
    "        plt.axhline(y=self.results_df['Accuracy'].median(), color='red', \n",
    "                   linestyle='--', alpha=0.5)\n",
    "        plt.axvline(x=self.results_df['Avg Processing Time (s)'].median(), \n",
    "                   color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"‚úÖ Accuracy vs speed plot saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"‚ùå No evaluation results available for confusion matrices\")\n",
    "            return\n",
    "        \n",
    "        models_with_predictions = [\n",
    "            model for model, results in self.evaluation_results.items()\n",
    "            if 'predictions' in results and 'ground_truth' in results\n",
    "        ]\n",
    "        \n",
    "        if not models_with_predictions:\n",
    "            print(\"‚ùå No prediction data available for confusion matrices\")\n",
    "            return\n",
    "        \n",
    "        n_models = len(models_with_predictions)\n",
    "        cols = min(3, n_models)\n",
    "        rows = (n_models + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        if n_models == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        fig.suptitle('Confusion Matrices by Model', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, model_name in enumerate(models_with_predictions):\n",
    "            row, col = i // cols, i % cols\n",
    "            ax = axes[row, col] if rows > 1 else axes[col]\n",
    "            \n",
    "            results = self.evaluation_results[model_name]\n",
    "            cm = confusion_matrix(results['ground_truth'], results['predictions'])\n",
    "            \n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                       xticklabels=['Different', 'Same'], \n",
    "                       yticklabels=['Different', 'Same'])\n",
    "            ax.set_title(f'{model_name}')\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('Actual')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(n_models, rows * cols):\n",
    "            row, col = i // cols, i % cols\n",
    "            if rows > 1:\n",
    "                axes[row, col].set_visible(False)\n",
    "            else:\n",
    "                axes[col].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"‚úÖ Confusion matrices saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def create_summary_table(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Create and display a formatted summary table\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"‚ùå No results to display\")\n",
    "            return\n",
    "        \n",
    "        # Style the dataframe for better presentation\n",
    "        styled_df = self.results_df.round(4).style.format({\n",
    "            'Accuracy': '{:.3f}',\n",
    "            'Precision': '{:.3f}',\n",
    "            'Recall': '{:.3f}',\n",
    "            'F1-Score': '{:.3f}',\n",
    "            'Avg Processing Time (s)': '{:.3f}',\n",
    "            'Error Rate': '{:.3f}'\n",
    "        }).background_gradient(subset=['Accuracy', 'Precision', 'Recall', 'F1-Score'], \n",
    "                              cmap='RdYlGn', vmin=0, vmax=1)\n",
    "        \n",
    "        display(styled_df)\n",
    "        \n",
    "        if save_path:\n",
    "            self.results_df.to_csv(save_path, index=False)\n",
    "            print(f\"‚úÖ Summary table saved to {save_path}\")\n",
    "    \n",
    "    def generate_all_visualizations(self, base_path: str = \"plots\"):\n",
    "        \"\"\"Generate all visualization plots\"\"\"\n",
    "        print(\"üé® Generating all visualizations...\")\n",
    "        \n",
    "        base_path = Path(base_path)\n",
    "        base_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Generate all plots\n",
    "        self.plot_performance_metrics(base_path / \"performance_metrics.png\")\n",
    "        self.plot_processing_time_comparison(base_path / \"processing_time.png\")\n",
    "        self.plot_accuracy_vs_speed(base_path / \"accuracy_vs_speed.png\")\n",
    "        self.plot_confusion_matrices(base_path / \"confusion_matrices.png\")\n",
    "        self.create_summary_table(base_path / \"summary_table.csv\")\n",
    "        \n",
    "        print(f\"‚úÖ All visualizations saved to {base_path}\")\n",
    "\n",
    "# Create visualizer and generate plots\n",
    "if not results_df.empty:\n",
    "    print(\"üé® Creating visualizations...\")\n",
    "    visualizer = ResultsVisualizer(results_df, evaluation_results)\n",
    "    \n",
    "    # Generate individual plots\n",
    "    print(\"\\nüìä Performance Metrics Comparison:\")\n",
    "    visualizer.plot_performance_metrics()\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è Processing Time Comparison:\")\n",
    "    visualizer.plot_processing_time_comparison()\n",
    "    \n",
    "    print(\"\\n‚ö° Accuracy vs Speed Trade-off:\")\n",
    "    visualizer.plot_accuracy_vs_speed()\n",
    "    \n",
    "    print(\"\\nüîç Confusion Matrices:\")\n",
    "    visualizer.plot_confusion_matrices()\n",
    "    \n",
    "    print(\"\\nüìã Summary Table:\")\n",
    "    visualizer.create_summary_table()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for visualization\")\n",
    "    print(\"   Please run the model evaluation first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4075f65",
   "metadata": {},
   "source": [
    "## 7. Model Fine-tuning Framework Setup\n",
    "\n",
    "Establish framework structure for future model fine-tuning capabilities. This section creates placeholder functions and configuration for custom training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb05a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing fine-tuning framework...\n",
      "‚úÖ Fine-tuning framework initialized!\n",
      "\n",
      "üìã Example Configuration:\n",
      "  - Model: VGG-Face\n",
      "  - Learning Rate: 0.0001\n",
      "  - Batch Size: 16\n",
      "  - Epochs: 5\n",
      "  - Data Splits: 0.7/0.15/0.15\n",
      "\n",
      "üí° To run fine-tuning, execute:\n",
      "    results = finetuning_framework.run_complete_finetuning_pipeline(example_config)\n",
      "\n",
      "‚ö†Ô∏è Note: This is a framework setup. Actual implementation requires deep learning libraries like TensorFlow/PyTorch.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class FineTuningConfig:\n",
    "    \"\"\"Configuration for model fine-tuning\"\"\"\n",
    "    model_name: str\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 10\n",
    "    train_split: float = 0.8\n",
    "    validation_split: float = 0.1\n",
    "    test_split: float = 0.1\n",
    "    augmentation: bool = True\n",
    "    freeze_layers: int = 0  # Number of layers to freeze\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    log_dir: str = \"logs\"\n",
    "\n",
    "class FineTuningFramework:\n",
    "    \"\"\"Framework for future model fine-tuning capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_loader: DatasetLoader):\n",
    "        self.dataset_loader = dataset_loader\n",
    "        self.models = {}\n",
    "        self.training_history = {}\n",
    "        \n",
    "    def prepare_dataset_for_training(self, config: FineTuningConfig) -> Dict:\n",
    "        \"\"\"Prepare dataset for training (placeholder implementation)\"\"\"\n",
    "        print(f\"üîÑ Preparing dataset for fine-tuning {config.model_name}...\")\n",
    "        \n",
    "        # Get image paths\n",
    "        image_paths = self.dataset_loader.get_image_paths()\n",
    "        \n",
    "        if not image_paths:\n",
    "            print(\"‚ùå No images found for training\")\n",
    "            return {}\n",
    "        \n",
    "        # Create identity labels from folder names\n",
    "        labels = []\n",
    "        valid_paths = []\n",
    "        \n",
    "        for path in image_paths:\n",
    "            person_name = Path(path).parent.name\n",
    "            if self.dataset_loader.validate_image(path):\n",
    "                labels.append(person_name)\n",
    "                valid_paths.append(path)\n",
    "        \n",
    "        # Convert labels to numeric\n",
    "        unique_labels = list(set(labels))\n",
    "        label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "        numeric_labels = [label_to_id[label] for label in labels]\n",
    "        \n",
    "        # Split dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            valid_paths, numeric_labels, \n",
    "            test_size=(1 - config.train_split), \n",
    "            random_state=42, \n",
    "            stratify=numeric_labels\n",
    "        )\n",
    "        \n",
    "        val_test_split = config.validation_split / (config.validation_split + config.test_split)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, \n",
    "            test_size=(1 - val_test_split), \n",
    "            random_state=42, \n",
    "            stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        dataset_info = {\n",
    "            'train': {'paths': X_train, 'labels': y_train},\n",
    "            'validation': {'paths': X_val, 'labels': y_val},\n",
    "            'test': {'paths': X_test, 'labels': y_test},\n",
    "            'num_classes': len(unique_labels),\n",
    "            'class_names': unique_labels,\n",
    "            'label_mapping': label_to_id\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Dataset prepared:\")\n",
    "        print(f\"  - Training samples: {len(X_train)}\")\n",
    "        print(f\"  - Validation samples: {len(X_val)}\")\n",
    "        print(f\"  - Test samples: {len(X_test)}\")\n",
    "        print(f\"  - Number of classes: {len(unique_labels)}\")\n",
    "        \n",
    "        return dataset_info\n",
    "    \n",
    "    def create_data_augmentation_pipeline(self, config: FineTuningConfig) -> object:\n",
    "        \"\"\"Create data augmentation pipeline (placeholder)\"\"\"\n",
    "        print(\"üîÑ Creating data augmentation pipeline...\")\n",
    "        \n",
    "        # Placeholder for data augmentation\n",
    "        # In practice, you might use libraries like albumentations or torchvision\n",
    "        augmentation_config = {\n",
    "            'horizontal_flip': True,\n",
    "            'rotation_range': 15,\n",
    "            'zoom_range': 0.1,\n",
    "            'brightness_range': 0.1,\n",
    "            'contrast_range': 0.1,\n",
    "            'gaussian_noise': 0.01\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Data augmentation pipeline created\")\n",
    "        return augmentation_config\n",
    "    \n",
    "    def setup_model_for_finetuning(self, config: FineTuningConfig, num_classes: int) -> Dict:\n",
    "        \"\"\"Setup model architecture for fine-tuning (placeholder)\"\"\"\n",
    "        print(f\"üîÑ Setting up {config.model_name} for fine-tuning...\")\n",
    "        \n",
    "        # Placeholder for model setup\n",
    "        # In practice, you would load the pre-trained model and modify the final layers\n",
    "        model_setup = {\n",
    "            'model_name': config.model_name,\n",
    "            'num_classes': num_classes,\n",
    "            'freeze_layers': config.freeze_layers,\n",
    "            'learning_rate': config.learning_rate,\n",
    "            'architecture': 'placeholder_architecture',\n",
    "            'status': 'configured'\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Model {config.model_name} configured for {num_classes} classes\")\n",
    "        return model_setup\n",
    "    \n",
    "    def train_model(self, config: FineTuningConfig, dataset_info: Dict) -> Dict:\n",
    "        \"\"\"Train the model (placeholder implementation)\"\"\"\n",
    "        print(f\"üöÄ Starting fine-tuning of {config.model_name}...\")\n",
    "        \n",
    "        # Placeholder for actual training loop\n",
    "        # In practice, this would implement the actual training process\n",
    "        \n",
    "        training_info = {\n",
    "            'model_name': config.model_name,\n",
    "            'epochs': config.epochs,\n",
    "            'batch_size': config.batch_size,\n",
    "            'learning_rate': config.learning_rate,\n",
    "            'training_samples': len(dataset_info['train']['paths']),\n",
    "            'validation_samples': len(dataset_info['validation']['paths']),\n",
    "            'status': 'training_placeholder',\n",
    "            'history': {\n",
    "                'train_loss': [0.8, 0.6, 0.4, 0.3, 0.2],  # Placeholder values\n",
    "                'val_loss': [0.9, 0.7, 0.5, 0.4, 0.3],\n",
    "                'train_accuracy': [0.6, 0.7, 0.8, 0.85, 0.9],\n",
    "                'val_accuracy': [0.5, 0.65, 0.75, 0.8, 0.85]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Fine-tuning completed for {config.model_name}\")\n",
    "        print(f\"  - Final training accuracy: {training_info['history']['train_accuracy'][-1]:.3f}\")\n",
    "        print(f\"  - Final validation accuracy: {training_info['history']['val_accuracy'][-1]:.3f}\")\n",
    "        \n",
    "        return training_info\n",
    "    \n",
    "    def evaluate_finetuned_model(self, model_name: str, dataset_info: Dict) -> Dict:\n",
    "        \"\"\"Evaluate fine-tuned model (placeholder)\"\"\"\n",
    "        print(f\"üìä Evaluating fine-tuned {model_name}...\")\n",
    "        \n",
    "        # Placeholder evaluation\n",
    "        evaluation_results = {\n",
    "            'model_name': model_name,\n",
    "            'test_accuracy': 0.87,  # Placeholder value\n",
    "            'test_precision': 0.85,\n",
    "            'test_recall': 0.88,\n",
    "            'test_f1': 0.86,\n",
    "            'confusion_matrix': 'placeholder',\n",
    "            'classification_report': 'placeholder'\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Evaluation completed for {model_name}\")\n",
    "        print(f\"  - Test Accuracy: {evaluation_results['test_accuracy']:.3f}\")\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def save_finetuned_model(self, model_name: str, model_info: Dict, save_path: str) -> str:\n",
    "        \"\"\"Save fine-tuned model (placeholder)\"\"\"\n",
    "        save_path = Path(save_path)\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        model_path = save_path / f\"{model_name}_finetuned.pkl\"\n",
    "        \n",
    "        # Placeholder for model saving\n",
    "        with open(model_path, 'w') as f:\n",
    "            json.dump(model_info, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved to {model_path}\")\n",
    "        return str(model_path)\n",
    "    \n",
    "    def run_complete_finetuning_pipeline(self, config: FineTuningConfig) -> Dict:\n",
    "        \"\"\"Run the complete fine-tuning pipeline\"\"\"\n",
    "        print(f\"üöÄ Starting complete fine-tuning pipeline for {config.model_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Step 1: Prepare dataset\n",
    "        dataset_info = self.prepare_dataset_for_training(config)\n",
    "        if not dataset_info:\n",
    "            return {'status': 'failed', 'reason': 'dataset_preparation_failed'}\n",
    "        \n",
    "        # Step 2: Create augmentation pipeline\n",
    "        augmentation = self.create_data_augmentation_pipeline(config)\n",
    "        \n",
    "        # Step 3: Setup model\n",
    "        model_setup = self.setup_model_for_finetuning(config, dataset_info['num_classes'])\n",
    "        \n",
    "        # Step 4: Train model\n",
    "        training_info = self.train_model(config, dataset_info)\n",
    "        \n",
    "        # Step 5: Evaluate model\n",
    "        evaluation_info = self.evaluate_finetuned_model(config.model_name, dataset_info)\n",
    "        \n",
    "        # Step 6: Save model\n",
    "        model_path = self.save_finetuned_model(\n",
    "            config.model_name, \n",
    "            {**model_setup, **training_info}, \n",
    "            config.checkpoint_dir\n",
    "        )\n",
    "        \n",
    "        complete_results = {\n",
    "            'status': 'completed',\n",
    "            'config': config,\n",
    "            'dataset_info': dataset_info,\n",
    "            'model_setup': model_setup,\n",
    "            'training_info': training_info,\n",
    "            'evaluation_info': evaluation_info,\n",
    "            'model_path': model_path,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"üéâ Fine-tuning pipeline completed successfully!\")\n",
    "        \n",
    "        return complete_results\n",
    "\n",
    "# Initialize fine-tuning framework\n",
    "if dataset_loader:\n",
    "    print(\"üîß Initializing fine-tuning framework...\")\n",
    "    finetuning_framework = FineTuningFramework(dataset_loader)\n",
    "    \n",
    "    # Example fine-tuning configuration\n",
    "    example_config = FineTuningConfig(\n",
    "        model_name=\"VGG-Face\",\n",
    "        learning_rate=0.0001,\n",
    "        batch_size=16,\n",
    "        epochs=5,\n",
    "        train_split=0.7,\n",
    "        validation_split=0.15,\n",
    "        test_split=0.15,\n",
    "        augmentation=True,\n",
    "        freeze_layers=10\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Fine-tuning framework initialized!\")\n",
    "    print(\"\\nüìã Example Configuration:\")\n",
    "    print(f\"  - Model: {example_config.model_name}\")\n",
    "    print(f\"  - Learning Rate: {example_config.learning_rate}\")\n",
    "    print(f\"  - Batch Size: {example_config.batch_size}\")\n",
    "    print(f\"  - Epochs: {example_config.epochs}\")\n",
    "    print(f\"  - Data Splits: {example_config.train_split}/{example_config.validation_split}/{example_config.test_split}\")\n",
    "    \n",
    "    print(\"\\nüí° To run fine-tuning, execute:\")\n",
    "    print(\"    results = finetuning_framework.run_complete_finetuning_pipeline(example_config)\")\n",
    "    print(\"\\n‚ö†Ô∏è Note: This is a framework setup. Actual implementation requires deep learning libraries like TensorFlow/PyTorch.\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize fine-tuning framework - dataset loader not available\")\n",
    "    finetuning_framework = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f363c7",
   "metadata": {},
   "source": [
    "## 8. Export Results and Model Artifacts\n",
    "\n",
    "Save evaluation results to CSV/JSON files, export trained models, and create summary reports for model comparison analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bc54bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No results available for export.\n",
      "   Please run the model evaluation first.\n",
      "\n",
      "================================================================================\n",
      "üéâ Facial Recognition Model Comparison Notebook Complete!\n",
      "\n",
      "üìã How to use with different datasets:\n",
      "   1. Change SELECTED_DATASET variable in cell 6 to:\n",
      "      - 'lfw' for LFW dataset (structured with CSV files)\n",
      "      - 'asian_celeb' for Asian Celebrity dataset (folder-based)\n",
      "   2. Re-run cells 6-12 to evaluate with the new dataset\n",
      "\n",
      "üìã Next Steps:\n",
      "   2. Check the 'plots' directory for visualizations.\n",
      "   3. Analyze the detailed CSV and JSON files for deeper insights.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "class ResultsExporter:\n",
    "    \"\"\"Export and save evaluation results and model artifacts\"\"\"\n",
    "    \n",
    "    def __init__(self, results_df: pd.DataFrame, evaluation_results: Dict, dataset_name: str = \"Unknown\", base_dir: str = \"results\"):\n",
    "        self.results_df = results_df\n",
    "        self.evaluation_results = evaluation_results\n",
    "        self.dataset_name = dataset_name\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "    def export_summary_csv(self) -> str:\n",
    "        \"\"\"Export summary results to CSV\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"‚ùå No results to export\")\n",
    "            return \"\"\n",
    "        \n",
    "        csv_path = self.base_dir / f\"model_comparison_summary_{self.timestamp}.csv\"\n",
    "        self.results_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Summary CSV exported to {csv_path}\")\n",
    "        return str(csv_path)\n",
    "    \n",
    "    def export_detailed_json(self) -> str:\n",
    "        \"\"\"Export detailed results to JSON\"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"‚ùå No detailed results to export\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Prepare JSON-serializable data\n",
    "        json_data = {\n",
    "            'metadata': {\n",
    "                'timestamp': self.timestamp,\n",
    "                'total_models': len(self.evaluation_results),\n",
    "                'dataset': self.dataset_name,\n",
    "                'evaluation_type': 'face_verification'\n",
    "            },\n",
    "            'summary': self.results_df.to_dict('records') if not self.results_df.empty else [],\n",
    "            'detailed_results': {}\n",
    "        }\n",
    "        \n",
    "        # Add detailed results (excluding non-serializable objects)\n",
    "        for model_name, results in self.evaluation_results.items():\n",
    "            serializable_results = {}\n",
    "            for key, value in results.items():\n",
    "                if key not in ['predictions', 'ground_truth', 'distances']:  # Skip large arrays\n",
    "                    if isinstance(value, (int, float, str, bool, list, dict)):\n",
    "                        serializable_results[key] = value\n",
    "                    else:\n",
    "                        serializable_results[key] = str(value)\n",
    "            json_data['detailed_results'][model_name] = serializable_results\n",
    "        \n",
    "        json_path = self.base_dir / f\"model_comparison_detailed_{self.timestamp}.json\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(json_data, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Detailed JSON exported to {json_path}\")\n",
    "        return str(json_path)\n",
    "    \n",
    "    def export_predictions_data(self) -> str:\n",
    "        \"\"\"Export prediction data for further analysis\"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"‚ùå No prediction data to export\")\n",
    "            return \"\"\n",
    "        \n",
    "        predictions_data = []\n",
    "        \n",
    "        for model_name, results in self.evaluation_results.items():\n",
    "            if 'predictions' in results and 'ground_truth' in results:\n",
    "                for i, (pred, truth) in enumerate(zip(results['predictions'], results['ground_truth'])):\n",
    "                    predictions_data.append({\n",
    "                        'model': model_name,\n",
    "                        'sample_id': i,\n",
    "                        'prediction': int(pred),\n",
    "                        'ground_truth': int(truth),\n",
    "                        'correct': bool(pred == truth),\n",
    "                        'distance': results.get('distances', [None])[i] if i < len(results.get('distances', [])) else None\n",
    "                    })\n",
    "        \n",
    "        if predictions_data:\n",
    "            predictions_df = pd.DataFrame(predictions_data)\n",
    "            csv_path = self.base_dir / f\"model_predictions_{self.timestamp}.csv\"\n",
    "            predictions_df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Predictions data exported to {csv_path}\")\n",
    "            return str(csv_path)\n",
    "        else:\n",
    "            print(\"‚ùå No prediction data found\")\n",
    "            return \"\"\n",
    "    \n",
    "    def create_comparison_report(self) -> str:\n",
    "        \"\"\"Create a comprehensive comparison report in Markdown\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"‚ùå No results available for report\")\n",
    "            return \"\"\n",
    "        \n",
    "        report_path = self.base_dir / f\"model_comparison_report_{self.timestamp}.md\"\n",
    "        \n",
    "        # Find best performing models for each metric\n",
    "        best_accuracy = self.results_df.loc[self.results_df['Accuracy'].idxmax()]\n",
    "        best_precision = self.results_df.loc[self.results_df['Precision'].idxmax()]\n",
    "        best_recall = self.results_df.loc[self.results_df['Recall'].idxmax()]\n",
    "        best_f1 = self.results_df.loc[self.results_df['F1-Score'].idxmax()]\n",
    "        fastest_model = self.results_df.loc[self.results_df['Avg Processing Time (s)'].idxmin()]\n",
    "        \n",
    "        report_content = f\"\"\"# Facial Recognition Model Comparison Report\n",
    "\n",
    "## Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report presents a comprehensive comparison of {len(self.results_df)} facial recognition models using the **{self.dataset_name}** dataset. The evaluation covers multiple performance metrics including accuracy, precision, recall, F1-score, and processing speed.\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "- **Dataset**: {self.dataset_name}\n",
    "- **Evaluation Type**: Face Verification\n",
    "- **Total Models Evaluated**: {len(self.results_df)}\n",
    "- **Evaluation Timestamp**: {self.timestamp}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Best Performing Models by Metric:\n",
    "\n",
    "- **Highest Accuracy**: {best_accuracy['Model']} ({best_accuracy['Accuracy']:.3f})\n",
    "- **Highest Precision**: {best_precision['Model']} ({best_precision['Precision']:.3f})\n",
    "- **Highest Recall**: {best_recall['Model']} ({best_recall['Recall']:.3f})\n",
    "- **Highest F1-Score**: {best_f1['Model']} ({best_f1['F1-Score']:.3f})\n",
    "- **Fastest Processing**: {fastest_model['Model']} ({fastest_model['Avg Processing Time (s)']:.3f}s)\n",
    "\n",
    "## Detailed Results\n",
    "\n",
    "### Performance Metrics Summary\n",
    "\n",
    "| Model | Accuracy | Precision | Recall | F1-Score | Avg Time (s) | Error Rate |\n",
    "|-------|----------|-----------|--------|----------|--------------|------------|\n",
    "\"\"\"\n",
    "        \n",
    "        # Add detailed results table\n",
    "        for _, row in self.results_df.iterrows():\n",
    "            report_content += f\"| {row['Model']} | {row['Accuracy']:.3f} | {row['Precision']:.3f} | {row['Recall']:.3f} | {row['F1-Score']:.3f} | {row['Avg Processing Time (s)']:.3f} | {row['Error Rate']:.3f} |\\n\"\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "### Statistical Analysis\n",
    "\n",
    "- **Mean Accuracy**: {self.results_df['Accuracy'].mean():.3f} ¬± {self.results_df['Accuracy'].std():.3f}\n",
    "- **Mean Precision**: {self.results_df['Precision'].mean():.3f} ¬± {self.results_df['Precision'].std():.3f}\n",
    "- **Mean Recall**: {self.results_df['Recall'].mean():.3f} ¬± {self.results_df['Recall'].std():.3f}\n",
    "- **Mean F1-Score**: {self.results_df['F1-Score'].mean():.3f} ¬± {self.results_df['F1-Score'].std():.3f}\n",
    "- **Mean Processing Time**: {self.results_df['Avg Processing Time (s)'].mean():.3f} ¬± {self.results_df['Avg Processing Time (s)'].std():.3f}s\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **For Highest Accuracy**: Use **{best_accuracy['Model']}** if accuracy is the primary concern.\n",
    "2. **For Speed**: Use **{fastest_model['Model']}** for real-time applications where latency is critical.\n",
    "3. **For Balanced Performance**: Consider **{best_f1['Model']}** for the best trade-off between precision and recall.\n",
    "\n",
    "### Dataset-Specific Notes\n",
    "\n",
    "{\"- This evaluation used the LFW (Labeled Faces in the Wild) dataset, which contains faces from web images with varying poses, lighting, and expressions.\" if self.dataset_name == \"LFW\" else \"\"}\n",
    "{\"- This evaluation used the Asian Celebrity dataset, which contains facial images of Asian celebrities organized by individual folders.\" if self.dataset_name == \"Asian Celebrity\" else \"\"}\n",
    "\n",
    "---\n",
    "\n",
    "*Report generated by the Facial Recognition Model Comparison Framework*\n",
    "\"\"\"\n",
    "        \n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        print(f\"‚úÖ Comparison report created: {report_path}\")\n",
    "        return str(report_path)\n",
    "    \n",
    "    def export_model_configurations(self) -> str:\n",
    "        \"\"\"Export model configurations for reproducibility\"\"\"\n",
    "        config_data = {\n",
    "            'timestamp': self.timestamp,\n",
    "            'dataset_config': {\n",
    "                'name': self.dataset_name,\n",
    "                'selected_dataset': SELECTED_DATASET,\n",
    "                'max_samples': CONFIG.get('max_samples'),\n",
    "                'test_size': CONFIG.get('test_size'),\n",
    "                'random_state': CONFIG.get('random_state')\n",
    "            },\n",
    "            'model_configurations': {},\n",
    "            'evaluation_settings': {\n",
    "                'distance_metric': 'cosine',\n",
    "                'enforce_detection': False,\n",
    "                'metrics_calculated': METRICS\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add model configurations\n",
    "        for model_name in available_models:\n",
    "            model_info = model_manager.get_model_info(model_name)\n",
    "            config_data['model_configurations'][model_name] = model_info\n",
    "        \n",
    "        config_path = self.base_dir / f\"model_configurations_{self.timestamp}.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config_data, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Model configurations exported to {config_path}\")\n",
    "        return str(config_path)\n",
    "    \n",
    "    def export_all_results(self) -> Dict[str, str]:\n",
    "        \"\"\"Export all results and create comprehensive output\"\"\"\n",
    "        print(\"üì¶ Exporting all results...\")\n",
    "        \n",
    "        exported_files = {}\n",
    "        \n",
    "        # Export all components\n",
    "        exported_files['summary_csv'] = self.export_summary_csv()\n",
    "        exported_files['detailed_json'] = self.export_detailed_json()\n",
    "        exported_files['predictions_csv'] = self.export_predictions_data()\n",
    "        exported_files['report_md'] = self.create_comparison_report()\n",
    "        exported_files['config_json'] = self.export_model_configurations()\n",
    "        \n",
    "        # Create index file\n",
    "        index_content = f\"\"\"# Facial Recognition Model Comparison Results\n",
    "\n",
    "## Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "This directory contains all the outputs from the model evaluation run using the **{self.dataset_name}** dataset.\n",
    "\n",
    "## Exported Files:\n",
    "\n",
    "- **Comparison Report**: [{Path(exported_files['report_md']).name}]({Path(exported_files['report_md']).name}) - A human-readable summary of the key findings and recommendations. **Start here!**\n",
    "- **Summary CSV**: [{Path(exported_files['summary_csv']).name}]({Path(exported_files['summary_csv']).name}) - Quick comparison of key performance metrics for all models.\n",
    "- **Predictions CSV**: [{Path(exported_files['predictions_csv']).name}]({Path(exported_files['predictions_csv']).name}) - Raw prediction data for in-depth error analysis.\n",
    "- **Detailed JSON**: [{Path(exported_files['detailed_json']).name}]({Path(exported_files['detailed_json']).name}) - Detailed results for programmatic analysis.\n",
    "- **Model Configurations**: [{Path(exported_files['config_json']).name}]({Path(exported_files['config_json']).name}) - Configuration details for ensuring reproducibility.\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "- **Dataset Used**: {self.dataset_name}\n",
    "- **Dataset Type**: {\"Folder-based (dynamic pair generation)\" if dataset_loader and dataset_loader.config.dataset_type == 'folder_based' else \"Structured (with predefined pairs)\"}\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by the Facial Recognition Model Comparison Framework*\n",
    "\"\"\"\n",
    "        \n",
    "        index_path = self.base_dir / f\"README_{self.timestamp}.md\"\n",
    "        with open(index_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(index_content)\n",
    "        \n",
    "        exported_files['index_md'] = str(index_path)\n",
    "        \n",
    "        print(f\"\\n‚úÖ All results exported to the '{self.base_dir}' directory.\")\n",
    "        print(f\"üìã Index file created: {index_path}\")\n",
    "        \n",
    "        return exported_files\n",
    "\n",
    "if not results_df.empty and dataset_loader:\n",
    "    print(\"=\"*80)\n",
    "    print(\"üì¶ Initializing Results Export...\")\n",
    "    \n",
    "    # Create an exporter instance with dataset name\n",
    "    dataset_name = dataset_loader.config.name if dataset_loader else \"Unknown\"\n",
    "    exporter = ResultsExporter(results_df, evaluation_results, dataset_name)\n",
    "    \n",
    "    # Export all results\n",
    "    exported_files = exporter.export_all_results()\n",
    "    \n",
    "    print(\"\\nüìä Export Summary:\")\n",
    "    for file_type, file_path in exported_files.items():\n",
    "        if file_path:\n",
    "            print(f\"  ‚úÖ {file_type.replace('_', ' ').title():<20}: {Path(file_path).name}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {file_type.replace('_', ' ').title():<20}: Failed to export\")\n",
    "    \n",
    "    # Also save to plots directory for visualizations\n",
    "    if 'visualizer' in locals():\n",
    "        print(\"\\nüé® Generating and saving all visualizations...\")\n",
    "        visualizer.generate_all_visualizations(\"plots\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for export.\")\n",
    "    print(\"   Please run the model evaluation first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Facial Recognition Model Comparison Notebook Complete!\")\n",
    "print(\"\\nüìã How to use with different datasets:\")\n",
    "print(\"   1. Change SELECTED_DATASET variable in cell 6 to:\")\n",
    "print(\"      - 'lfw' for LFW dataset (structured with CSV files)\")\n",
    "print(\"      - 'asian_celeb' for Asian Celebrity dataset (folder-based)\")\n",
    "print(\"   2. Re-run cells 6-12 to evaluate with the new dataset\")\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "if 'exporter' in locals():\n",
    "    print(f\"   1. Review the generated comparison report in the '{exporter.base_dir}' directory.\")\n",
    "print(\"   2. Check the 'plots' directory for visualizations.\")\n",
    "print(\"   3. Analyze the detailed CSV and JSON files for deeper insights.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ed7df",
   "metadata": {},
   "source": [
    "# Exporting Deepface models to tensorflow backend (tfgo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db34f6",
   "metadata": {},
   "source": [
    "## Tensorflow based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5769e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VGG-Face model exported to SavedModel format at: models/VGG-Face_savedmodel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from deepface import DeepFace \n",
    "model_name = \"VGG-Face\"  # Example model name, replace with actual model name as needed\n",
    "model = DeepFace.build_model(model_name).model\n",
    "\n",
    "# Export to TensorFlow SavedModel format\n",
    "saved_model_dir = f\"models/{model_name}_savedmodel\"\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "print(f\"‚úÖ {model_name} model exported to SavedModel format at: {saved_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bfa7007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GhostFaceNet model exported to SavedModel format at: models/GhostFaceNet_savedmodel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from deepface import DeepFace \n",
    "model_name = \"GhostFaceNet\"  # Example model name, replace with actual model name as needed\n",
    "model = DeepFace.build_model(model_name).model\n",
    "\n",
    "# Export to TensorFlow SavedModel format\n",
    "saved_model_dir = f\"models/{model_name}_savedmodel\"\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "print(f\"‚úÖ {model_name} model exported to SavedModel format at: {saved_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b60996b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Facenet model exported to SavedModel format at: models/Facenet_savedmodel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from deepface import DeepFace \n",
    "model_name = \"Facenet\"  # Example model name, replace with actual model name as needed\n",
    "model = DeepFace.build_model(model_name).model\n",
    "\n",
    "# Export to TensorFlow SavedModel format\n",
    "saved_model_dir = f\"models/{model_name}_savedmodel\"\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "print(f\"‚úÖ {model_name} model exported to SavedModel format at: {saved_model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
